<p align=center><img src=https://d31uz8lwfmyn8g.cloudfront.net/Assets/logo-henry-white-lg.png><p>

# <h1 align=center> **PROYECTO INDIVIDUAL Nº1** </h1>

# <h1 align=center>**`Data Engineering`**</h1>

<p align="center">
<img src="https://files.realpython.com/media/What-is-Data-Engineering_Watermarked.607e761a3c0e.jpg"  height=300>
</p>

¡Bienvenidos a mi primer proyecto individual! En esta ocasión, te voy a mostrar el paso a paso del proceso de ETL que realice en donde se ingestaron los datos desde diversas fuentes, como *csv* o *json*, para aplicar las transformaciones que consideren pertinentes, y luego disponibilizar los datos limpios para su consulta a través de una API construida en un entorno virtual dockerizado.

Los datos serán provistos en archivos de diferentes extensiones, como *csv* o *json*. Se espera que realicen un EDA para cada dataset y corrijan los tipos de datos, valores nulos y duplicados, entre otras tareas. Posteriormente, tendrán que relacionar los datasets así pueden acceder a su información por medio de consultas a la API.

<hr>  

## **Proceso de ETL**

1) Se ingestan los datos provistos en la carpeta Datasets, en el repositorio.<sup>*</sup>
2) Se realiza la limpieza de datos para poder unificar los archivos en un solo Data Frame.
3) 

## **Creacion de la API**




## **Fuente de datos**


